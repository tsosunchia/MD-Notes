流式训练的天花板是批式训练吗？批式训练的效果真的更好吗？

流式训练和批式训练怎么共同使用的？批流一体又是啥？

https://volcano.sh/zh/blog/xiaohongshu/



https://developer.aliyun.com/article/783112 Flink 执行引擎：流批一体的融合之路



商业现实已经发生了变化，所以现在更快做出的决定更有价值





精通Java核心，熟悉JDK各种集合，多线程，锁机制相关底层结构及原理，熟悉各种垃圾回收算法，有JVM调优实战经验。

熟练应用常用的数据结构与算法。

熟悉NIO，epoll相关的常见IO模型以及优化策略。

熟悉MySQL存储引擎，索引、事务原理。

熟悉Redis内存模型，Redis缓存常见问题，有Redis cluster集群搭建经验。

熟悉消息队列Kafka的原理及使用。

对Linux内核源码有过研究，对Docker等容器虚拟化技术有一定的了解。

了解分布式系统设计方案及原理，CAP定理，Paxos协议。

会用 Spark、Flink、HDFS进行大数据处理，阅读过相关论文，理解其设计原理。





字节跳动	推荐系统架构研发实习生	2021.01～2021.05

（1）推荐系统服务组件接入

在极光医疗接入字节推荐系统体系期间，负责流式训练样本的拼接，为id系统平滑迁移提供支持。为特征工程开发提供支持，包括阅读时长、阅读完成度等特征加入。在样本拼接场景中，系统缓存大量失效user action导致频繁GC会带来业务数据处理能力的降低，提出了解决方案：

(a)存储序列化后的protobuf对象，减少小对象数量。

(b)使用G1回收器，减少应用暂停时间。

另外，由于Flink任务的并行特性，针对单TaskManager内多个slot共享同一JVM导致创建多个metrics实例的问题，（任务抛异常，去Flink TM页面的对应TM log找到了抛异常的内容，是公司内部的Metrics库抛异常说名称重复，本地standalone模式运行查不出来无法复现，因为只有1个TM。后来去官方文档看了Flink的设计结构，是单个TM有多个slot导致的）于是使用double check lock单例+volatile关键字，保证单个TaskManager内仅创建1个metrics实例。实际上Fourier早已考虑到这个问题。

为抖音电商部门推荐系统训练样本增加静态特征提供系统的解决方案。跨部门协作期间，在每日TB级数据量业务的情况下，提升了在应对新需求时分析风险、进行合理技术选型的能力，并对Flink有了更深的理解，主要涉及：

(a)network shuffle, network buffer 

(b)parallelism 

(c)state backends.

在参与抖音服务端消重系统从pyFlink到FlinkSQL迁移的过程中，通过聚合请求、RPC异步调用，提高了吞吐量。使用火焰图（分为内存火焰图和调用时间火焰图，、jstat、jmap等工具排查OOM问题，对JVM内存分配有了更深入的理解。

（2）平台效率方面

参与电商部门容量大盘建设，梳理服务间调用关系，用于维护整个系统链路稳定性。

参与国际化部门项目代码配置文件的拆分、校验、上线，方便后续独立迭代。



其它

- 聚合请求
- 



##### 各种OOM

1、依赖的一个库中有内存泄露

FlinkSQL消重服务启动之后，JVM堆外内存一直涨，超出了容器分配的内存，然后去官网看了Flink的一些配置参数，里面提到了一些启动时默认大小、ratio等等，包括**堆外内存主要用来做什么的**。也问了组里的同学，mentor，结果都没有遇到过，后来拉了oncall，oncall的同学专门做基于flink的二次开发，发了他自己的文档，包括一些猛如虎的数学推理（根据DAG的出度和入度个数、parallelism，TM个数，JM个数计算需要的network buffer大小），我也根据他的方法算了下，调了 flink 的启动参数，结果有所缓解，不至于启动之后立马挂掉，但仍然会缓慢的内存泄漏。觉得事情没那么简单，于是开始二分查 bug，就先人肉运维，定期重启。因为服务挂了会自动拉起，因为 flink 有 state backened 的机制用于容错（这里可以DDIA展开了讲），恢复之前的一个“正确”的状态，实际上不停的重启，不停的消费同一个数据，导致 hotkey 报警。（这里DDIA可以聊：怎么避免重复消费）

2、yarn 队列，4000 核心，15 TB内存，可以讲一下 shuffle 机制以及排查过程（这里DDIA可以聊broadcast）



##### DDIA 临时笔记

- 关于 lag：输出的消息数量应该等于输入的消息数量，如果发现差异立刻告警

- p95，p99，p999 为什么关注更大的百分位数？
  - 直接影响用户的总体服务体验
  - 请求最慢的客户往往因为购买的更多的商品，是最有价值的客户
  - 要在客户端来测响应时间
  
- 把 **无状态** 服务分布然后拓展至多台机器相对比较容易，而 **有状态** 服务从单个节点扩展到分布式多机环境的复杂性会大大增加

- 简化复杂度
  - 高级编程语言作为一种抽象，可以隐藏机器汇编代码、CPU 寄存器和系统调用等细节和复杂性
  - SQL 作为一种抽象，隐藏了内部复杂的磁盘和内存数据结构，以及来自客户端的并发请求，系统崩溃之后的不一致问题
  - 不同的场景适用于不同的语言，例如 web，数据库 适合声明式，应用程序逻辑适合命令式，mapreduce 是一个相当底层的编程模型，它的操作必须是纯函数（没有副作用，也是因此保证了重试机制下写 hdfs 文件的唯一性，否则会有重复消费、不幂等问题）
  
- 数据库：适当的索引可以加速查询，但每个索引都会减慢写速度（牺牲写性能，增加读性能）

- LSM-Tree：大日志被分段、归并，顺序写比随机写快得多；日志结构化在后台执行所有合并，不会干扰前端查询
  - 磁盘上维护排序结构：B-tree
  
  - 内存中维护排序结构：红黑树、AVL 树
  
  - LSM-Tree 通常对于写入更快，而 B-tree 被认为对于读取更快
  
  - 彩蛋：在许多 SSD 上，固件内部使用日志结构化算法将随机写入转换为底层存储芯片上的顺序写入，所以存储引擎写入模式的影响不那么明显。
  
    >《Skip List...》论文理解：skip list 和 B-Tree 如何选型？
    >
    >左神：首先，从算法时间复杂度上，大家都是一样的，区别只有常数项。然后针对 **磁盘 / 内存** 读写，有各自的优化。
    >
    >无论 B/B+，RB，AVL，还是 skiplist，在最开始都实际上面临同一个问题，在数据有序的情况下，都会成为一个链表。最后导致查询复杂度 O(N)， B/B+/RB/AVL 的采用的策略是 self adjust ，就是所谓的 rebalance。跳表的策略是做 randomize，他假定了一个概率 P，有一个“退化”描述，即，当 P =1/2 时，有百分之50%的数据在第一层，百分之25的数据在第二层，百分之12.5的数据在第三层 etc. 通过这样让数据分层明显，避免全局退化。所以在这种情况下 skiplist 在期望复杂度上做到了查询 O(logn)（实际上这也是一个统计上的期望计算）。skiplist 的优势也很明显，实现简单，性能不错，但是缺陷也很明显，在内存的情况下，**由于其是个 randomize 的数据结构，导致其 CPU cache的利用率并不是很明显**，所以可能要考虑这块的场合还是需要去用 RB 这种数据结构。在磁盘 I/O 的情况下，**由于其 randomize 的特性，导致你没法去做 Sequential reading ，而 randomize reading 对于磁盘来说一直是个大问题**。在这个问题上 RB 这种数据结构实际上面临的问题也是一样的，所以 B/B+ 树用适当的数据聚合的方式，将树的高度降下来，方面我们做 page read，避免 randomize reading
    >
    >DDIA P81：B树的设计更接近底层硬件，因为磁盘也是以固定大小的块排列
    >
    >DDIA P84：许多B-tree的实现尝试对树进行布局，以便相邻叶子page可以按顺序保存在磁盘上。相比之下，skip list 更随机位置，性能更差
    >
    >DDIA P89：尽管 Redis 写入磁盘，但磁盘的作用仅仅用作为了持久性目的的追加日志，读取完全靠内存服务（写磁盘时，通过异步写入磁盘提供较弱的持久性）
    >
    >与直觉相反，内存数据库的性能优势并不是因为它们不需要从磁盘读取（如果内存足够大，OS会将最近使用磁盘块缓存在内存中）。内存数据库可以更快，是因为它们避免使用写磁盘的格式对内存结构编码的开销。
  
- 主从复制

  - 从节点失效：追赶式恢复
  - 主节点失效：节点切换
    - 确认主节点失效（超时机制）
    - 选举新的主节点（选举）
    - 重新配置使新的主节点生效（并且防止旧的主节点脑裂）

- 强一致性 > 单调读一致性 > 最终一致性

- 多主节点复制：横跨多个数据中心。

  - 数据中心之间通过广域网连接，主主之间通常采用异步复制。相比之下，pacificA 在局域网中，适合同步复制
  - 初衷：就近访问

- 无主节点复制：

  - 写操作：由 client 直接发送给其它节点。
  - 读操作：由 client 同时向多个节点发送读请求，会同时读到新数据和旧数据。通过 Read repair（由读client主动去发现并且更新那些旧数据） / anti-entropy process（有一个后台线程不停的扫描 diff 并来回复制） 解决。


